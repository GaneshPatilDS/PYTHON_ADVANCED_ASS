{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1b20930-a987-479d-afa0-962bd6f72ee8",
   "metadata": {},
   "source": [
    "## QUESTIONS :\n",
    "\n",
    "1. Is it OK to initialize all the weights to the same value as long as that value is selected\n",
    "   randomly using He initialization?\n",
    "   \n",
    "2. Is it OK to initialize the bias terms to 0?\n",
    "\n",
    "3. Name three advantages of the SELU activation function over ReLU.\n",
    "\n",
    "4. In which cases would you want to use each of the following activation functions: SELU, leaky\n",
    "   ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "   \n",
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999)\n",
    "   when using an SGD optimizer?\n",
    "   \n",
    "6. Name three ways you can produce a sparse model.\n",
    "\n",
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
    "   new instances)? What about MC Dropout?\n",
    "   \n",
    "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "\n",
    "   a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the\n",
    "   point of this exercise). Use He initialization and the ELU activation function.\n",
    "   \n",
    "   \n",
    "   b. Using Nadam optimization and early stopping, train the network on the CIFAR10\n",
    "   dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is\n",
    "   composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for\n",
    "   testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.\n",
    "   Remember to search for the right learning rate each time you change the model’s\n",
    "   architecture or hyperparameters.\n",
    "   \n",
    "   \n",
    "   c. Now try adding Batch Normalization and compare the learning curves: Is it\n",
    "   converging faster than before? Does it produce a better model? How does it affect\n",
    "   training speed?\n",
    "   \n",
    "   \n",
    "   d. Try replacing Batch Normalization with SELU, and make the necessary adjustements\n",
    "   to ensure the network self-normalizes (i.e., standardize the input features, use\n",
    "   LeCun normal initialization, make sure the DNN contains only a sequence of dense\n",
    "   layers, etc.).\n",
    "   \n",
    "   e. Try regularizing the model with alpha dropout. Then, without retraining your model,\n",
    "   see if you can achieve better accuracy using MC Dropout.\n",
    "   \n",
    "   -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cea214-42e8-47c4-bc7e-913efb00629a",
   "metadata": {},
   "source": [
    "## ANS :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff37772d-1bdc-42be-a4a3-76a9048fa75f",
   "metadata": {},
   "source": [
    "1. Initializing all weights to the same value, even if randomly selected with He initialization, is not recommended. The purpose of He initialization is to break the symmetry of weights, allowing each neuron to learn different features. If all weights are set to the same value, this symmetry is not broken, and the network may not learn effectively.\n",
    "\n",
    "2. Initializing bias terms to 0 is generally acceptable. Biases help the neurons to activate, and initializing them to 0 allows the network to start learning with no specific bias. However, some networks may benefit from non-zero bias initialization in certain cases.\n",
    "\n",
    "3. Three advantages of the SELU activation function over ReLU are:\n",
    "   a. It addresses the vanishing/exploding gradient problem.\n",
    "   b. It is designed to be self-normalizing, helping maintain a consistent scale of activations.\n",
    "   c. It allows for better gradient flow, potentially leading to faster convergence.\n",
    "\n",
    "4. Use cases for activation functions:\n",
    "   - SELU: Suitable for deep networks, helps with vanishing/exploding gradients.\n",
    "   - Leaky ReLU and variants: Good for addressing dying ReLU problem, allowing a small gradient for negative values.\n",
    "   - ReLU: Often a good default choice, computationally efficient.\n",
    "   - Tanh: Useful in the middle layers of a neural network to center and scale the data.\n",
    "   - Logistic (Sigmoid): Typically used in binary classification output layers.\n",
    "   - Softmax: Appropriate for multi-class classification output layers.\n",
    "\n",
    "5. Setting the momentum hyperparameter too close to 1 (e.g., 0.99999) in SGD can lead to slow convergence or oscillations around the minimum. It can cause the optimizer to overshoot the minimum and take longer to converge.\n",
    "\n",
    "6. Three ways to produce a sparse model:\n",
    "   a. **L1 Regularization**: Encourages sparse weight matrices by penalizing large weights.\n",
    "   b. **Dropout**: Randomly drops connections during training, effectively creating a sparse network.\n",
    "   c. **Pruning**: Remove connections or neurons based on their importance, often done after training.\n",
    "\n",
    "7. Dropout can slow down training, but it helps prevent overfitting. During inference, dropout is typically turned off, so it does not affect the prediction speed. MC Dropout involves running the model multiple times with dropout enabled and averaging the predictions, which can be slower than a single prediction but helps capture uncertainty.\n",
    "\n",
    "8. Answers for training a deep neural network on the CIFAR10 dataset:\n",
    "\n",
    "   a. Here's a basic code snippet using TensorFlow/Keras:\n",
    "   ```python\n",
    "   import tensorflow as tf\n",
    "   from tensorflow.keras import layers, models, optimizers\n",
    "   from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "   # Load CIFAR10 dataset\n",
    "   (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "   # Build DNN\n",
    "   model = models.Sequential()\n",
    "   model.add(layers.Flatten(input_shape=(32, 32, 3)))\n",
    "   for _ in range(20):\n",
    "       model.add(layers.Dense(100, kernel_initializer='he_normal', activation='elu'))\n",
    "\n",
    "   model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "   # b. Compile and train with Nadam optimization and early stopping\n",
    "   model.compile(optimizer=optimizers.Nadam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "   early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "   history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "   # c. Add Batch Normalization\n",
    "   # Note: You need to modify the model architecture for Batch Normalization\n",
    "   # (e.g., add BN layers after Dense layers), and then compare learning curves.\n",
    "\n",
    "   # d. Replace Batch Normalization with SELU\n",
    "   # Note: Adjust the model architecture, input standardization, and initialization.\n",
    "   # Ensure the DNN contains only a sequence of dense layers.\n",
    "\n",
    "   # e. Regularize with alpha dropout and try MC Dropout\n",
    "   # Note: Add AlphaDropout layers, and for MC Dropout, run multiple predictions with dropout enabled and average results.\n",
    "   ```\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d4462-9672-4195-a23b-b0f366097515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
